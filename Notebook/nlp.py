# -*- coding: utf-8 -*-
"""nlp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sWuQcOeUOaxoYm3F8eEZjTTG0brRRLjj
"""

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop, SGD
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score

!pip install datasets

#from datasets import load_dataset

#train_ds, val_ds, test_ds = load_dataset(
    #'allocine',
    #split=['train', 'validation', 'test']
#)

from datasets import load_dataset
# Charger le jeu de données
train_ds, val_ds, test_ds = load_dataset('allocine', split=['train', 'validation', 'test'])
# Convertir le jeu de données d'entraînement en DataFrame pandas
train_df = train_ds.to_pandas()
# Afficher les premières lignes du DataFrame
print(train_df.head())

train_df

def preprocess_text(text):
    # Convertir le texte en minuscules
    text = text.lower()

    # Supprimer la ponctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Supprimer les chiffres
    text = re.sub(r'\d+', '', text)

    # Tokenization
    tokens = word_tokenize(text)

    # Supprimer les stopwords
    stop_words = set(stopwords.words('french'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Lemmatisation
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

    # Rejoindre les tokens pour former du texte prétraité
    preprocessed_text = ' '.join(lemmatized_tokens)

    return preprocessed_text

# Appliquer la fonction de prétraitement sur la colonne 'review'
train_df['review_processed'] = train_df['review'].apply(preprocess_text)

train_df.head()

# Charger les données
train_ds, val_ds, test_ds = load_dataset('allocine', split=['train', 'validation', 'test'])

# Prétraiter chaque texte dans l'ensemble de données d'entraînement
train_texts_preprocessed = [preprocess_text(text) for text in train_ds['review']]

# Afficher quelques exemples de texte prétraité
for i in range(5):
    print("Texte original :", train_ds['review'][i])
    print("Texte prétraité :", train_texts_preprocessed[i])
    print()

# Extraire les données d'entraînement et d'étiquettes
train_texts = train_ds['review']
train_labels = train_ds['label']


# Vectorisation des données avec Bag-of-Words
vectorizer_bow = CountVectorizer()
train_features_bow = vectorizer_bow.fit_transform(train_texts)

# Vectorisation des données avec TF-IDF
vectorizer_tfidf = TfidfVectorizer()
train_features_tfidf = vectorizer_tfidf.fit_transform(train_texts)

# Diviser les données d'entraînement en train/validation
# Diviser les données en train/validation
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)
train_features_bow, val_features_bow = train_test_split(train_features_bow, test_size=0.2, random_state=42)
train_features_tfidf, val_features_tfidf = train_test_split(train_features_tfidf, test_size=0.2, random_state=42)

# Entraîner et évaluer Logistic Regression avec Bag-of-Words
lr_bow = LogisticRegression()
lr_bow.fit(train_features_bow, train_labels)
val_pred_lr_bow = lr_bow.predict(val_features_bow)
accuracy_lr_bow = accuracy_score(val_labels, val_pred_lr_bow)
print("Accuracy of Logistic Regression with Bag-of-Words:", accuracy_lr_bow)

# Entraîner et évaluer Logistic Regression avec TF-IDF
lr_tfidf = LogisticRegression()
lr_tfidf.fit(train_features_tfidf, train_labels)
val_pred_lr_tfidf = lr_tfidf.predict(val_features_tfidf)
accuracy_lr_tfidf = accuracy_score(val_labels, val_pred_lr_tfidf)
print("Accuracy of Logistic Regression with TF-IDF:", accuracy_lr_tfidf)

# Entraîner et évaluer SVM avec Bag-of-Words
svm_bow = SVC()
svm_bow.fit(train_features_bow, train_labels)
val_pred_svm_bow = svm_bow.predict(val_features_bow)
accuracy_svm_bow = accuracy_score(val_labels, val_pred_svm_bow)
print("Accuracy of SVM with Bag-of-Words:", accuracy_svm_bow)

# Entraîner et évaluer SVM avec TF-IDF
svm_tfidf = SVC()
svm_tfidf.fit(train_features_tfidf, train_labels)
val_pred_svm_tfidf = svm_tfidf.predict(val_features_tfidf)
accuracy_svm_tfidf = accuracy_score(val_labels, val_pred_svm_tfidf)
print("Accuracy of SVM with TF-IDF:", accuracy_svm_tfidf)

import numpy as np

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from datasets import load_dataset
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from sklearn.metrics import accuracy_score

# Téléchargement des ressources NLTK
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Prétraitement du texte
def preprocess_text(text):
    text = text.lower()  # Convertir en minuscules
    text = text.translate(str.maketrans('', '', string.punctuation))  # Supprimer la ponctuation
    text = re.sub(r'\d+', '', text)  # Supprimer les chiffres
    tokens = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words('french'))  # Charger les stopwords
    filtered_tokens = [word for word in tokens if word not in stop_words]  # Supprimer les stopwords
    lemmatizer = WordNetLemmatizer()  # Initialiser le lemmatizer
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatisation
    preprocessed_text = ' '.join(lemmatized_tokens)  # Joindre les tokens
    return preprocessed_text

# Charger les données
train_ds, val_ds, test_ds = load_dataset('allocine', split=['train', 'validation', 'test'])

# Prétraiter chaque texte dans l'ensemble de données d'entraînement
train_texts_preprocessed = [preprocess_text(text) for text in train_ds['review']]
val_texts_preprocessed = [preprocess_text(text) for text in val_ds['review']]
test_texts_preprocessed = [preprocess_text(text) for text in test_ds['review']]

# Tokenization et padding
max_len = 100  # Longueur maximale des séquences
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(train_texts_preprocessed)
train_sequences = tokenizer.texts_to_sequences(train_texts_preprocessed)
train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')
val_sequences = tokenizer.texts_to_sequences(val_texts_preprocessed)
val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')
test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)
test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')

# Création du modèle LSTM
embedding_dim = 100
model = Sequential([
    Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

# Compilation du modèle
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entraînement du modèle
epochs = 5
batch_size = 64
history = model.fit(train_padded, train_ds['label'], epochs=epochs, batch_size=batch_size, validation_data=(val_padded, val_ds['label']), verbose=2)

# Évaluation du modèle sur l'ensemble de test
test_loss, test_accuracy = model.evaluate(test_padded, test_ds['label'], verbose=0)
print("Accuracy on test set:", test_accuracy)

# Prédiction sur l'ensemble de test
test_predictions = (model.predict(test_padded) > 0.5).astype("int32")
print("Accuracy score on test set:", accuracy_score(test_ds['label'], test_predictions))

# Définir la longueur maximale des séquences et la dimension d'embedding
max_length = 100
embedding_dim = 100

# Padding des séquences pour qu'elles aient toutes la même longueur
train_sequences = pad_sequences(train_texts_encoded, maxlen=max_length)

# Construire le modèle LSTM
model = Sequential()
model.add(Embedding(input_dim=len(word2vec_model.vocab), output_dim=embedding_dim, input_length=max_length))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=1, activation='sigmoid'))

# Compiler le modèle
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entraîner le modèle
model.fit(train_sequences, train_labels, epochs=10, batch_size=64, validation_split=0.2)

# Prétraiter et encoder les textes de validation
val_sequences = pad_sequences([encode_text_with_word2vec(text, word2vec_model, embedding_dim) for text in val_texts], maxlen=max_length)

# Évaluer le modèle sur les données de validation
val_pred = model.predict_classes(val_sequences)
accuracy_lstm = accuracy_score(val_labels, val_pred)
print("Accuracy of LSTM model:", accuracy_lstm)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.optimizers import RMSprop, SGD
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score

# Définir la longueur maximale des séquences et la dimension d'embedding
max_length = 100
embedding_dim = 100

# Padding des séquences pour qu'elles aient toutes la même longueur
train_sequences = pad_sequences(train_texts_encoded, maxlen=max_length)

# Construire le modèle LSTM avec l'optimiseur RMSprop
model_rmsprop = Sequential()
model_rmsprop.add(Embedding(input_dim=len(word2vec_model.vocab), output_dim=embedding_dim, input_length=max_length))
model_rmsprop.add(SpatialDropout1D(0.2))
model_rmsprop.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model_rmsprop.add(Dense(units=1, activation='sigmoid'))

# Compiler le modèle avec l'optimiseur RMSprop
model_rmsprop.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])

# Entraîner le modèle avec l'optimiseur RMSprop
model_rmsprop.fit(train_sequences, train_labels, epochs=10, batch_size=64, validation_split=0.2)

# Construire le modèle LSTM avec l'optimiseur SGD
model_sgd = Sequential()
model_sgd.add(Embedding(input_dim=len(word2vec_model.vocab), output_dim=embedding_dim, input_length=max_length))
model_sgd.add(SpatialDropout1D(0.2))
model_sgd.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model_sgd.add(Dense(units=1, activation='sigmoid'))

# Compiler le modèle avec l'optimiseur SGD
model_sgd.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])

# Entraîner le modèle avec l'optimiseur SGD
model_sgd.fit(train_sequences, train_labels, epochs=10, batch_size=64, validation_split=0.2)

# Prétraiter et encoder les textes de validation
val_sequences = pad_sequences([encode_text_with_word2vec(text, word2vec_model, embedding_dim) for text in val_texts], maxlen=max_length)

# Évaluer le modèle avec l'optimiseur RMSprop sur les données de validation
val_pred_rmsprop = model_rmsprop.predict_classes(val_sequences)
accuracy_lstm_rmsprop = accuracy_score(val_labels, val_pred_rmsprop)
print("Accuracy of LSTM model with RMSprop optimizer:", accuracy_lstm_rmsprop)

# Évaluer le modèle avec l'optimiseur SGD sur les données de validation
val_pred_sgd = model_sgd.predict_classes(val_sequences)
accuracy_lstm_sgd = accuracy_score(val_labels, val_pred_sgd)
print("Accuracy of LSTM model with SGD optimizer:", accuracy_lstm_sgd)

es noms de modèles disponibles peuvent varier en fonction de la version de Gensim et des mises à jour de la bibliothèque. Cependant, Gensim propose généralement une sélection de modèles pré-entraînés pour différentes langues, y compris le français.

Pour trouver les noms de modèles disponibles pour le français dans la documentation de Gensim ou sur leur site web officiel, vous pouvez suivre ces étapes :

    Visitez le site web officiel de Gensim à l'adresse suivante : https://radimrehurek.com/gensim/index.html
    Recherchez la documentation ou la section des modèles pré-entraînés.
    Consultez les informations spécifiques aux modèles disponibles pour le français.
    Identifiez les noms de modèles appropriés qui peuvent être utilisés pour charger un modèle Word2Vec pré-entraîné pour le français.

Vous pouvez également parcourir la documentation de Gensim ou consulter des ressources supplémentaires telles que des forums de discussion, des publications académiques ou des références en ligne pour obtenir des informations sur les modèles disponibles et leurs noms.

Gardez à l'esprit que les noms de modèles peuvent changer ou être mis à jour, donc il est toujours bon de consulter les ressources officielles pour obtenir les informations les plus récentes.

from sklearn.ensemble import RandomForestClassifier

# Initialiser et entraîner le modèle de forêt aléatoire
rf_classifier_bow = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier_bow.fit(X_train_counts, df_train['label'])

# Faire des prédictions sur l'ensemble de test
rf_predictions_bow = rf_classifier_bow.predict(X_test_counts)

# Évaluer les performances du modèle
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(df_test['label'], rf_predictions_bow)
print("Accuracy:", accuracy)

print("Classification Report:")
print(classification_report(df_test['label'], rf_predictions_bow))

# Régression logistique avec BoW
lr_classifier_bow = LogisticRegression()
lr_classifier_bow.fit(X_train_counts, df_train['label'])
lr_predictions_bow = lr_classifier_bow.predict(X_test_counts)

print("Régression logistique avec Bag-of-Words :")
print("Accuracy:", accuracy_score(df_test['label'], lr_predictions_bow))
print("Classification Report:")
print(classification_report(df_test['label'], lr_predictions_bow))

# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['preprocessed_text'])
X_test_tfidf = tfidf_vectorizer.transform(df_test['preprocessed_text'])

# Régression logistique avec TF-IDF
lr_classifier_tfidf = LogisticRegression()
lr_classifier_tfidf.fit(X_train_tfidf, df_train['label'])
lr_predictions_tfidf = lr_classifier_tfidf.predict(X_test_tfidf)


print("Régression logistique avec TF-IDF :")
print("Accuracy:", accuracy_score(df_test['label'], lr_predictions_tfidf))
print("Classification Report:")
print(classification_report(df_test['label'], lr_predictions_tfidf))

# Accéder aux ensembles d'entraînement, de validation et de test
train_dataset = dataset['train']
valid_dataset = dataset['validation']
test_dataset = dataset['test']

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from datasets import load_dataset
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from sklearn.metrics import accuracy_score

# Téléchargement des ressources NLTK
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Prétraitement du texte
def preprocess_text(text):
    text = text.lower()  # Convertir en minuscules
    text = text.translate(str.maketrans('', '', string.punctuation))  # Supprimer la ponctuation
    text = re.sub(r'\d+', '', text)  # Supprimer les chiffres
    tokens = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words('french'))  # Charger les stopwords
    filtered_tokens = [word for word in tokens if word not in stop_words]  # Supprimer les stopwords
    lemmatizer = WordNetLemmatizer()  # Initialiser le lemmatizer
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatisation
    preprocessed_text = ' '.join(lemmatized_tokens)  # Joindre les tokens
    return preprocessed_text

# Charger les données
train_ds, val_ds, test_ds = load_dataset('allocine', split=['train', 'validation', 'test'])

# Prétraiter chaque texte dans l'ensemble de données d'entraînement
train_texts_preprocessed = [preprocess_text(text) for text in train_ds['review']]
val_texts_preprocessed = [preprocess_text(text) for text in val_ds['review']]
test_texts_preprocessed = [preprocess_text(text) for text in test_ds['review']]

# Tokenization et padding
max_len = 100  # Longueur maximale des séquences
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(train_texts_preprocessed)
train_sequences = tokenizer.texts_to_sequences(train_texts_preprocessed)
train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')
val_sequences = tokenizer.texts_to_sequences(val_texts_preprocessed)
val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')
test_sequences = tokenizer.texts_to_sequences(test_texts_preprocessed)
test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')

# Convertir les étiquettes en tableaux Numpy
train_labels = np.array(train_ds['label'])
val_labels = np.array(val_ds['label'])
test_labels = np.array(test_ds['label'])

# Création du modèle LSTM
embedding_dim = 100
model = Sequential([
    Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

# Compilation du modèle
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entraînement du modèle
epochs = 5
batch_size = 64
history = model.fit(train_padded, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(val_padded, val_labels), verbose=2)

# Évaluation du modèle sur l'ensemble de test
test_loss, test_accuracy = model.evaluate(test_padded, test_labels, verbose=0)
print("Accuracy on test set:", test_accuracy)

# Prédiction sur l'ensemble de test
test_predictions = (model.predict(test_padded) > 0.5).astype("int32")
print("Accuracy score on test set:", accuracy_score(test_labels, test_predictions))

#Création du modèle LSTM avec l'optimiseur RMSprop
embedding_dim = 100
model_rmsprop = Sequential([
    Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])
model_rmsprop.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# Entraînement du modèle avec l'optimiseur RMSprop
model_rmsprop.fit(train_padded, train_labels, epochs=5, batch_size=64, validation_data=(val_padded, val_labels), verbose=2)

# Création du modèle LSTM avec l'optimiseur SGD
model_sgd = Sequential([
    Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_len),
    SpatialDropout1D(0.2),
    LSTM(100, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])
model_sgd.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])

# Entraînement du modèle avec l'optimiseur SGD
model_sgd.fit(train_padded, train_labels, epochs=5, batch_size=64, validation_data=(val_padded, val_labels), verbose=2)

# Évaluation des modèles sur l'ensemble de test
test_loss_rmsprop, test_accuracy_rmsprop = model_rmsprop.evaluate(test_padded, test_labels, verbose=0)
print("Accuracy on test set with RMSprop optimizer:", test_accuracy_rmsprop)

test_loss_sgd, test_accuracy_sgd = model_sgd.evaluate(test_padded, test_labels, verbose=0)
print("Accuracy on test set with SGD optimizer:", test_accuracy_sgd)