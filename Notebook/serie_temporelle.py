# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JSej83njX4OT13Yz7NbslUSaNwNuI_Yf

# Importation des packages
"""

!pip install openmeteo-requests
!pip install requests-cache retry-requests numpy pandas

"""# Scrapping du jeu de données"""

import openmeteo_requests

import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
openmeteo = openmeteo_requests.Client(session = retry_session)

# Make sure all required weather variables are listed here
# The order of variables in hourly or daily is important to assign them correctly below
url = "https://archive-api.open-meteo.com/v1/archive"
params = {
	"latitude": 52.52,
	"longitude": 13.41,
	"start_date": "2024-03-21",
	"end_date": "2024-04-04",
	"hourly": "temperature_2m"
}
responses = openmeteo.weather_api(url, params=params)

# Process first location. Add a for-loop for multiple locations or weather models
response = responses[0]
print(f"Coordinates {response.Latitude()}°N {response.Longitude()}°E")
print(f"Elevation {response.Elevation()} m asl")
print(f"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}")
print(f"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s")

# Process hourly data. The order of variables needs to be the same as requested.
hourly = response.Hourly()
hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()

hourly_data = {"date": pd.date_range(
	start = pd.to_datetime(hourly.Time(), unit = "s", utc = True),
	end = pd.to_datetime(hourly.TimeEnd(), unit = "s", utc = True),
	freq = pd.Timedelta(seconds = hourly.Interval()),
	inclusive = "left"
)}
hourly_data["temperature_2m"] = hourly_temperature_2m

hourly_dataframe = pd.DataFrame(data = hourly_data)
print(hourly_dataframe)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Import des bibliothèques OpenMeteo et autres dépendances
import openmeteo_requests
import requests_cache
from retry_requests import retry
import matplotlib.pyplot as plt


# Configuration de la session de cache et de réessai
cache_session = requests_cache.CachedSession('.cache', expire_after=-1)
retry_session = retry(cache_session, retries=5, backoff_factor=0.2)
openmeteo = openmeteo_requests.Client(session=retry_session)

# Paramètres de la requête OpenMeteo pour les données météorologiques
url = "https://archive-api.open-meteo.com/v1/archive"
params = {
    "latitude": 3.848,
    "longitude": 11.502,
    "start_date": "2020-01-01",
    "end_date": "2023-12-31",
    "hourly": "temperature_2m"
}

# Récupération des données météorologiques
responses = openmeteo.weather_api(url, params=params)

# Traitement des données météorologiques
response = responses[0]  # Première localisation
hourly = response.Hourly()
hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()

# Création de la série temporelle initiale
hourly_data = {
    "date": pd.date_range(
        start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
        end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
        freq=pd.Timedelta(seconds=hourly.Interval()),
        inclusive = "left"
    ),
    "temperature_2m": hourly_temperature_2m
}
hourly_dataframe = pd.DataFrame(data=hourly_data)

# Sauvegarde des données originales dans un fichier CSV
hourly_dataframe.to_csv('donnees_meteo_originales.csv', index=False)

# Transformation de la série temporelle selon les spécifications
# La valeur à chaque pas de temps de 3 heures est la moyenne des valeurs à ce pas de temps, plus les deux suivants
transformed_hourly_dataframe = hourly_dataframe.resample('3H', on='date').mean()

# Sauvegarde des données transformées dans un fichier CSV
transformed_hourly_dataframe.to_csv('donnees_meteo_transformees.csv', index=False)

# Tracé de la série temporelle transformée
plt.figure(figsize=(10, 6))
plt.plot(transformed_hourly_dataframe.index, transformed_hourly_dataframe['temperature_2m'], color='blue')
plt.title('Série temporelle de la température à 2m (Transformée)')
plt.xlabel('Date')
plt.ylabel('Température (°C)')
plt.grid(True)
plt.show()

import os

# Créer le répertoire s'il n'existe pas
if not os.path.exists('C:/TP/Data'):
    os.makedirs('C:/TP/Data')

# Ensuite, vous pouvez sauvegarder les fichiers
hourly_dataframe.to_csv('C:/TP/Data/donnees_meteo_originales.csv', index=False)
transformed_hourly_dataframe.to_csv('C:/TP/Data/donnees_meteo_transformees.csv', index=False)

"""



























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































# Décomposition de la série"""

# Décomposition saisonnière de la série temporelle
decomposition = sm.tsa.seasonal_decompose(transformed_hourly_dataframe['temperature_2m'], model='additive')

# Affichage des composantes de la décomposition saisonnière
plt.figure(figsize=(12, 8))

plt.subplot(4, 1, 1)
plt.plot(transformed_hourly_dataframe.index, decomposition.observed, label='Observed', color='blue')
plt.legend()
plt.ylabel('Observed')

plt.subplot(4, 1, 2)
plt.plot(transformed_hourly_dataframe.index, decomposition.trend, label='Trend', color='green')
plt.legend()
plt.ylabel('Trend')

plt.subplot(4, 1, 3)
plt.plot(transformed_hourly_dataframe.index, decomposition.seasonal, label='Seasonal', color='red')
plt.legend()
plt.ylabel('Seasonal')

plt.subplot(4, 1, 4)
plt.plot(transformed_hourly_dataframe.index, decomposition.resid, label='Residuals', color='purple')
plt.legend()
plt.ylabel('Residuals')

plt.xlabel('Date')
plt.tight_layout()
plt.show()

"""**Données Brutes :** La première partie montre les données de température réelles, qui fluctuent entre environ 20 et 30 degrés Celsius. Cela reflète la variabilité directe des températures enregistrées.

**Tendance :** La deuxième partie illustre la tendance des données. Il y a des fluctuations visibles, mais aucune tendance claire à la hausse ou à la baisse sur la période entière. Cela peut indiquer que la température moyenne n'a pas changé de manière significative sur cette période.

**Saisonnalité :** La troisième partie montre le composant saisonnier, indiquant une régularité dans les variations de température tout au long de l'année. Cela pourrait correspondre à des variations saisonnières typiques, comme des températures plus élevées en été et plus basses en hiver.

**Résidus :** La quatrième et dernière partie affiche les résidus, qui sont les écarts des données brutes par rapport à la tendance et à la saisonnalité. Les résidus semblent être aléatoires et sans motif apparent, ce qui est typique des composants irréguliers ou du "bruit" dans les données.

# Nature de la série
"""

# Tracé de la série temporelle transformée
plt.figure(figsize=(10, 6))
plt.plot(transformed_hourly_dataframe.index, transformed_hourly_dataframe['temperature_2m'], color='blue')

# Ajout de lignes droites pour montrer la nature additive
# Calcul de la tendance linéaire
z = np.polyfit(transformed_hourly_dataframe.index.astype(np.int64), transformed_hourly_dataframe['temperature_2m'], 1)
p = np.poly1d(z)

# Tracé de la tendance linéaire
plt.plot(transformed_hourly_dataframe.index, p(transformed_hourly_dataframe.index.astype(np.int64)), "r--")

# Ajout de lignes parallèles à la tendance pour visualiser la variabilité saisonnière
plt.fill_between(transformed_hourly_dataframe.index, p(transformed_hourly_dataframe.index.astype(np.int64)) - 5, p(transformed_hourly_dataframe.index.astype(np.int64)) + 5, color='green', alpha=0.3)

plt.title('Série temporelle de la température à 2m (Transformée)')
plt.xlabel('Date')
plt.ylabel('Température (°C)')
plt.grid(True)
plt.show()

"""# Test de la saisonnalité"""

from statsmodels.tsa import stattools

# Augmented Dickey-Fuller test
def check_adf_stat(ts):

  adf_results = stattools.adfuller(ts)
  adf_stat = adf_results[0]
  p_value = adf_results[1]
  critical_values = adf_results[4]

  print(f'The Augmented Dickey-Fuller test results\
          \n  test statistic {adf_stat:.3f},\
          \n  p-value: {p_value:.3f}\
          \n  critical values: {critical_values}')

  if (p_value > 0.05) or (adf_stat > critical_values['5%']):
    # there is a unit root
    print('The time series is NOT stationary')
  else:
    # there is no unit roots
    print('The time series is stationary')

warnings.filterwarnings('ignore')
check_adf_stat(transformed_hourly_dataframe)

# Kwiatkowski–Phillips–Schmidt–Shin test
def check_kpss_stat(ts):

  kpss_results = stattools.kpss(ts)
  kpss_stat = kpss_results[0]
  p_value = kpss_results[1]
  critical_values = kpss_results[3]

  print(f'The KPSS test results\
        \n  test statistic {kpss_stat:.3f},\
        \n  p-value: {p_value:.3f}\
        \n  critical values: {critical_values}')

  if (p_value > 0.05) or (kpss_stat < critical_values['5%']):
      print('The time series is stationary')
  else:
      print('The time series is NOT stationary')

warnings.filterwarnings('ignore')
check_kpss_stat(transformed_hourly_dataframe)

"""**Au vu de ces données on constate que notre n'est pas stationnaire** Nous allons utiliser plusieurs méthodes pour essayer de désosonaliser notre serie.

* On va Utiliser la fonction diff() de python
* On va essayer faire une stabilisation nous même en procédant comme suit: $
X_t - X_{t-1}$

* On va utiliser les méthodes de box et jeskins
"""

plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})
# Original Series
fig, (ax1, ax2, ax3) = plt.subplots(3)
ax1.plot(transformed_hourly_dataframe); ax1.set_title('Original Series'); ax1.axes.xaxis.set_visible(False)
# 1st Differencing
ax2.plot(transformed_hourly_dataframe.diff()); ax2.set_title('1st Order Differencing'); ax2.axes.xaxis.set_visible(False)
# 2nd Differencing
ax3.plot(transformed_hourly_dataframe.diff().diff()); ax3.set_title('2nd Order Differencing')
plt.show()

"""**Série Originale :** Ce graphique montre la série temporelle initiale avec une tendance et une saisonnalité visibles, ce qui suggère que la série n'est pas stationnaire.

**Différenciation d'Ordre 1 :** Ici, une première différenciation a été appliquée, ce qui signifie que la tendance a été retirée. Les fluctuations sont moins prononcées, indiquant que la série est plus proche de la stationnarité.

**Différenciation d'Ordre 2 :** Une deuxième différenciation semble avoir été appliquée, stabilisant davantage la variance et rendant la série encore plus stationnaire.

"""

# Check stationarity test stats
check_adf_stat(transformed_hourly_dataframe.diff().diff().dropna())
print("*"*20)
check_kpss_stat(transformed_hourly_dataframe.diff().diff().dropna())

# Check stationarity test stats
check_adf_stat(transformed_hourly_dataframe.diff().dropna())
print("*"*20)
check_kpss_stat(transformed_hourly_dataframe.diff().dropna())

import statsmodels.graphics.tsaplots as tsaplots
ts_data_arma = transformed_hourly_dataframe.diff().dropna()
fig = plt.figure(figsize=(25,10))
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_acf(ts_data_arma, lags=40, ax=ax1, title="ACF: Stationarized TS (Box-Cox + diff)");

import statsmodels.graphics.tsaplots as tsaplots
ts_data_arma = transformed_hourly_dataframe.diff().diff().dropna()
fig = plt.figure(figsize=(25,10))
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_acf(ts_data_arma, lags=40, ax=ax1, title="ACF: Stationarized TS (Box-Cox + diff)");

"""On constate que notre serie bien qu'elle soit stationnaire elle contient la saisonnalité il faut rétirer cela pour enlever la stationnalité et capter bien le mouvement de la serie"""

import statsmodels.graphics.tsaplots as tsaplots
ts_data_arma = transformed_hourly_dataframe.diff(56).dropna()
fig = plt.figure(figsize=(25,10))
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_acf(ts_data_arma, lags=40, ax=ax1, title="ACF: Stationarized TS (Box-Cox + diff)");

fig = plt.figure(figsize=(25,10))
ts_data_arma = transformed_hourly_dataframe.diff(56).dropna()
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_pacf(transformed_hourly_dataframe.diff(56).dropna(), lags=40, ax=ax1, title="PACF: Stationarized TS (diff + sdiff)");

import statsmodels.graphics.tsaplots as tsaplots
ts_data_arma = transformed_hourly_dataframe.diff().diff(56).dropna()
fig = plt.figure(figsize=(25,10))
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_acf(ts_data_arma, lags=40, ax=ax1, title="ACF: Stationarized TS (Box-Cox + diff)");

fig = plt.figure(figsize=(25,10))
ts_data_arma = transformed_hourly_dataframe.diff().diff(56).dropna()
ax1 = fig.add_subplot(211)
fig = tsaplots.plot_pacf(transformed_hourly_dataframe.diff(56).dropna(), lags=40, ax=ax1, title="PACF: Stationarized TS (diff + sdiff)");

"""# ARIMA (p,q,d)"""

# Split data into train (80%) and test (20%) sets
ind_split = int(len(transformed_hourly_dataframe) * 0.8)

ts_train = transformed_hourly_dataframe[:ind_split]
ts_test = transformed_hourly_dataframe[ind_split:]

transformed_hourly_dataframe.shape, ts_train.shape, ts_test.shape

from statsmodels.tsa.arima.model import ARIMA

# p,d,q
order=(6,1,10)
#order=(6,2,4)
model_1 = ARIMA(ts_train, order=order).fit()
print(model_1.summary())

# Effectuer des prédictions
predictions = model_1.predict(start=len(ts_train), end=len(ts_train) + len(ts_test) - 1, typ='levels')

# Comparer les prédictions avec les valeurs réelles
# Assurez-vous que ts_test a un index de date/heure pour que cela fonctionne correctement
comparison_df = ts_test.copy()
comparison_df['predictions'] = predictions

# Afficher les prédictions et les valeurs réelles
print(comparison_df)

from statsmodels.graphics.tsaplots import plot_predict

fig, ax = plt.subplots()
ax = ts_test.plot(ax=ax)
fig = plot_predict(model_1, start=ts_test.index[0], end=ts_test.index[-1], dynamic=False, ax=ax)
legend = ax.legend(loc="upper left")

from statsmodels.tsa.arima.model import ARIMA

# p,d,q
#order=(6,1,10)
order=(6,2,4)
model = ARIMA(ts_train, order=order).fit()
print(model.summary())

# Effectuer des prédictions
predictions = model.predict(start=len(ts_train), end=len(ts_train) + len(ts_test) - 1, typ='levels')

# Comparer les prédictions avec les valeurs réelles
# Assurez-vous que ts_test a un index de date/heure pour que cela fonctionne correctement
comparison_df = ts_test.copy()
comparison_df['predictions'] = predictions

# Afficher les prédictions et les valeurs réelles
print(comparison_df)



"""# Turning ARIMA paramètre"""

# From PACF plot
p_values = range(0,7)
# From ACF plot
q_values = range(0,11)
# We saw that single differencing transformed the ts into a stationary one
d_values = range(0,3)

# check if the same result can be obtained with ARIMA
from statsmodels.tsa.arima.model import ARIMA

best_order_aic_score_1, best_order_aic_cfg_1 = float("inf"), None

for p in p_values:
    for d in d_values:
        for q in q_values:
            order = (p,d,q)
            try:
                model = ARIMA(ts_train, order=order).fit()
                if model.aic < best_order_aic_score_1:
                    best_order_aic_score_1, best_order_aic_cfg_1 = model.aic, order
                    print(f'ARIMA{order} AIC={model.aic:.3f}')
            except:
                continue

print(f'Best ARIMA{best_order_aic_cfg_1} AIC={best_order_aic_score_1:.3f}')

"""# SARIMA(p,d,q)(P,D,Q)s"""

import statsmodels.api as sm



# Ajustement du modèle ARIMA sur les données d'entraînement
model = sm.tsa.arima.ARIMA(ts_train, order=(6, 2, 4), seasonal_order=(1, 1, 1, 12), trend='n', method='css-mle')
results = model.fit()

# Résumé du modèle
print(results.summary())

# Affichage des formes des ensembles de données
print(transformed_hourly_dataframe.shape, ts_train.shape, ts_test.shape)

"""# Regression"""

# Add columns to the dataframe for lags from 1 to 7
for i in range(1, 8):
    transformed_hourly_dataframe[f"lag_{i}"] = transformed_hourly_dataframe['temperature_2m'].shift(i)

transformed_hourly_dataframe.head()

# Drop rows containing NaN: only first max_lag (7 in this case) rows are concerned.
# These data cannot be used in a regression model.
transformed_hourly_dataframe.dropna(inplace=True)

transformed_hourly_dataframe.head()

X = transformed_hourly_dataframe.drop('temperature_2m', axis=1)
y = transformed_hourly_dataframe['temperature_2m']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)
X_train.shape, X_test.shape

# Check the chronology
X_train.index.min(), X_train.index.max(), X_test.index.min(), X_test.index.max()

split_date = '2022-12-31'

X_train = X.loc[:split_date]
y_train = y.loc[:split_date]
X_test = X.loc[split_date:]
y_test = y.loc[split_date:]

X_train.shape, X_test.shape

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)

plt.figure(figsize=(10, 5))
plt.plot(y_pred, "r", label="prediction")
plt.plot(y_test.values, label="actual")
plt.grid(True)
plt.legend(loc="best")
#plt.title(f"Linear regression\n Mean absolute percentage error {mean_absolute_percentage_error(y_test, y_pred)*100:.2f}%");

from sklearn.model_selection import TimeSeriesSplit
import numpy as np
errors = []

tscv = TimeSeriesSplit(n_splits=5)

for train_index, test_index in tscv.split(X):
    X_train, X_test = X.values[train_index], X.values[test_index]
    y_train, y_test = y.values[train_index], y.values[test_index]
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    errors.append(mean_absolute_percentage_error(y_test, y_pred))

np.mean(errors)

from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error
import numpy as np

errors = []

tscv = TimeSeriesSplit(n_splits=5)

for train_index, test_index in tscv.split(X):
    X_train, X_test = X.values[train_index], X.values[test_index]
    y_train, y_test = y.values[train_index], y.values[test_index]
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    errors.append(mean_absolute_percentage_error(y_test, y_pred))

mean_error = np.mean(errors)
print(mean_error)

# Function that returns a dictionary where:
# the keys correspond to unique values of `cat_feature`, and
# the values correspond to average values of `real_feature`.
def get_mean_by_cat(data, cat_feature, value_feature):
    return dict(data.groupby(cat_feature)[value_feature].mean())

# Add month and year columns by extracting the corresponding parts from the index
transformed_hourly_dataframe["month"] = transformed_hourly_dataframe.index.month
transformed_hourly_dataframe["year"] = transformed_hourly_dataframe.index.year

transformed_hourly_dataframe.head()

get_mean_by_cat(transformed_hourly_dataframe, "month", 'temperature_2m')

get_mean_by_cat(transformed_hourly_dataframe, "year", 'temperature_2m')

def preprocess_data(data, lag_start=1, lag_end=8, date_cut='31.12.2014'):

    data = pd.DataFrame(data.copy())

    # add time series lags as features
    for i in range(lag_start, lag_end):
        data[f"lag_{i}"] = data[value_column].shift(i)

    # drop NaNs generated by adding lag features
    data = data.dropna()

    data["month"] = data.index.month

    # calculate average values only on train data to avoid data leak
    data["month_average"] = list(map(get_mean_by_cat(data.loc[:date_cut], "month", value_column).get, data.month))

    # drop features that have been used for calculating average values of the target variable
    data.drop(["month"], axis=1, inplace=True)

    # split dataset on train and test parts
    X = data.drop([value_column], axis=1)
    y = data[value_column]
    X_train = X.loc[:date_cut]
    y_train = y.loc[:date_cut]
    X_test = X.loc[date_cut:]
    y_test = y.loc[date_cut:]

    return X_train, X_test, y_train, y_test

"""# SARIMAX"""

import openmeteo_requests

import requests_cache
import pandas as pd
from retry_requests import retry

# Setup the Open-Meteo API client with cache and retry on error
cache_session = requests_cache.CachedSession('.cache', expire_after = -1)
retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)
openmeteo = openmeteo_requests.Client(session = retry_session)

# Make sure all required weather variables are listed here
# The order of variables in hourly or daily is important to assign them correctly below
url = "https://archive-api.open-meteo.com/v1/archive"
params = {
	"latitude": 3.848,
  "longitude": 11.502,
  "start_date": "2020-01-01",
  "end_date": "2023-12-31",
	"hourly": ["temperature_2m", "apparent_temperature"]
}

responses = openmeteo.weather_api(url, params=params)

# Process first location. Add a for-loop for multiple locations or weather models
response = responses[0]
print(f"Coordinates {response.Latitude()}°N {response.Longitude()}°E")
print(f"Elevation {response.Elevation()} m asl")
print(f"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}")
print(f"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s")

# Process hourly data. The order of variables needs to be the same as requested.
hourly = response.Hourly()
hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()
hourly_apparent_temperature = hourly.Variables(1).ValuesAsNumpy()

hourly_data = {"date": pd.date_range(
	start = pd.to_datetime(hourly.Time(), unit = "s", utc = True),
	end = pd.to_datetime(hourly.TimeEnd(), unit = "s", utc = True),
	freq = pd.Timedelta(seconds = hourly.Interval()),
	inclusive = "left"
)}
hourly_data["temperature_2m"] = hourly_temperature_2m
hourly_data["apparent_temperature"] = hourly_apparent_temperature

hourly_dataframe = pd.DataFrame(data = hourly_data)
print(hourly_dataframe)

# From PACF plot
p_values = range(0,12)
# From ACF plot
q_values = range(0,12)
# We saw that single differencing transformed the ts into a stationary one
d_values = range(0,2)

import statsmodels.api as smfrom statsmodels.tsa.statespace.sarimax import SARIMAX

best_order_aic_score, best_order_aic_cfg = float("inf"), None

for p in p_values:
    for d in d_values:
        for q in q_values:
            order = (p,d,q)
            try:
                model = SARIMAX(ts_train, order=order).fit()
                if model.aic < best_order_aic_score:
                    best_order_aic_score, best_order_aic_cfg = model.aic, order
                    print(f'SARIMAX{order} AIC={model.aic:.3f}')
            except:
                continue

print(f'Best SARIMAX{best_order_aic_cfg} AIC={best_order_aic_score:.3f}')

# Votre série temporelle 'y'
y = ...

# Ajustement du modèle ARIMA
model1 = sm.tsa.arima.ARIMA(y, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12), trend='n', method='css-mle')
results = model1.fit()

# Résumé du modèle
print(results.summary())